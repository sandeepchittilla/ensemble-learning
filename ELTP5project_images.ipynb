{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ImMqxs8NB3w"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taneja/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/taneja/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/taneja/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/taneja/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/taneja/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/taneja/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/taneja/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/taneja/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/taneja/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/taneja/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/taneja/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/taneja/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns',None)\n",
    "import os\n",
    "import zipfile\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import skimage.transform\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.layers.core import Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mFOtwd-khGUK"
   },
   "outputs": [],
   "source": [
    "# Image settings \n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "IMG_CHANNELS = 3\n",
    "IMAGES_PATH = './images/image_train/' # why was it image_test before ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wh0hJIJ1Pv12"
   },
   "outputs": [],
   "source": [
    "def get_images_train_data(img_paths, output_shape=(None, None)):\n",
    "    try:\n",
    "        return(np.load(IMAGES_PATH + 'training_images.npy'))\n",
    "    except:\n",
    "        X_data = np.array([skimage.transform.resize(plt.imread((IMAGES_PATH + path + '.jpg'))[:,:,:3], \n",
    "                                                        output_shape=output_shape, mode='constant', \n",
    "                                                      preserve_range=True) for path in img_paths], \n",
    "                            dtype=np.uint8)\n",
    "\n",
    "          # let's save the numpy images array for faster loading\n",
    "        np.save(IMAGES_PATH + 'training_images.npy', X_data)\n",
    "        return X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading training and test csv\n",
    "X_train = pd.read_csv('X_train.csv', header=0, names=['id','designation','description','productid','imageid'])\n",
    "y_train = pd.read_csv('Y_train.csv', header=0, names=['id','prdtypecode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "B0KOdlpz5AaB",
    "outputId": "c59b381e-4841-459e-cc9b-40e3cf33c0af",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>imagefilename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>image_1263597046_product_3804725264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>image_1008141237_product_436067568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>image_938777978_product_201115110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "      <td>image_457047496_product_50418756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>Luc a des id&amp;eacute;es de grandeur. Il veut or...</td>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "      <td>image_1077757786_product_278535884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        designation  \\\n",
       "0   0  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1   1  Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...   \n",
       "2   2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "3   3  Peluche Donald - Europe - Disneyland 2000 (Mar...   \n",
       "4   4                               La Guerre Des Tuques   \n",
       "\n",
       "                                         description   productid     imageid  \\\n",
       "0                                                NaN  3804725264  1263597046   \n",
       "1                                                NaN   436067568  1008141237   \n",
       "2  PILOT STYLE Touch Pen de marque Speedlink est ...   201115110   938777978   \n",
       "3                                                NaN    50418756   457047496   \n",
       "4  Luc a des id&eacute;es de grandeur. Il veut or...   278535884  1077757786   \n",
       "\n",
       "                         imagefilename  \n",
       "0  image_1263597046_product_3804725264  \n",
       "1   image_1008141237_product_436067568  \n",
       "2    image_938777978_product_201115110  \n",
       "3     image_457047496_product_50418756  \n",
       "4   image_1077757786_product_278535884  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build filenames in the string format : product_123_image_456 from X_train\n",
    "X_train['imagefilename'] = X_train.apply(lambda x: 'image_'+str(x['imageid'])+'_product_'+str(x['productid']), axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsampling data\n",
    "\n",
    "def subSample(df, maxCount):\n",
    "    prd_dict = dict()\n",
    "    \n",
    "    for i in df.prdtypecode.value_counts().index:\n",
    "        prd_dict[i]=0\n",
    "    \n",
    "    ranks = []\n",
    "    for i in df.prdtypecode:\n",
    "        prd_dict[i] = prd_dict[i] +1\n",
    "        ranks.append(prd_dict[i])\n",
    "    \n",
    "    df['rank'] = ranks\n",
    "    temp = df[df['rank']<=maxCount]\n",
    "    temp.drop(columns=['rank'],inplace=True)\n",
    "    return temp\n",
    "\n",
    "def mapPrdIdtoInteger(df, col_name): # function to map prdTypeCode to Integer \n",
    "    uniqueProdIds = [i for i in pd.unique(df[col_name].values)]\n",
    "    prdIdDict = {}\n",
    "    for i in range(len(uniqueProdIds)):\n",
    "        prdIdDict.setdefault(uniqueProdIds[i],i)\n",
    "    return prdIdDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "y_train_subsampled = subSample(y_train.copy(), 500)\n",
    "subsampled_ids = y_train_subsampled['id']\n",
    "\n",
    "temp_df = pd.DataFrame(y_train_subsampled['prdtypecode'])\n",
    "\n",
    "map_dict = mapPrdIdtoInteger(temp_df,'prdtypecode') # encoding map \n",
    "inverse_map_dict = dict((v,k) for k,v in map_dict.items()) # decoding map for getting back actual ProductId\n",
    "\n",
    "y_train_subsampled = pd.DataFrame.replace(temp_df, to_replace = map_dict)\n",
    "\n",
    "X_train_subsampled = X_train.loc[subsampled_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rg3Y-SGX52EC"
   },
   "outputs": [],
   "source": [
    "# now let's read all the image filenames\n",
    "image_filenames = [f for f in X_train_subsampled['imagefilename']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading all training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4ba901701a48528fc42c04a85a8dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sample_length = len(X_train_subsampled)\n",
    "\n",
    "train_images_np=np.zeros((sample_length,IMG_HEIGHT,IMG_WIDTH,3))\n",
    "\n",
    "for i,fi in tqdm(enumerate(image_filenames)):\n",
    "    pic = cv2.imread(IMAGES_PATH + fi + '.jpg')\n",
    "    pic = cv2.resize(pic,(IMG_HEIGHT,IMG_WIDTH))\n",
    "    train_images_np[i] = pic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Axes swapping from h,w,c -> c,h,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13500, 3, 128, 128)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment below if using Pytorch\n",
    "train_images_np = np.moveaxis(train_images_np, -1, 1)\n",
    "train_images_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "wAGANv_W1ObZ",
    "outputId": "1dfddfd3-299a-43e6-92cb-a1a2e362c33d"
   },
   "source": [
    "### Train Test Split of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZaNLV3ONw6Y"
   },
   "outputs": [],
   "source": [
    "n = int(round(0.8*len(X_train_subsampled),0))\n",
    "\n",
    "y_train_subsampled = y_train_subsampled.prdtypecode.values\n",
    "\n",
    "x_train_cv = train_images_np[:n]\n",
    "x_val = train_images_np[n:]\n",
    "y_train_cv = y_train_subsampled[:n]\n",
    "y_val = y_train_subsampled[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# this cell takes a lot of time and increases with input size\n",
    "\n",
    "# transforming the numpy array to a torch tensor\n",
    "tensor_X_train = torch.Tensor(x_train_cv)\n",
    "tensor_Y_train = torch.Tensor(y_train_cv).type(torch.LongTensor)\n",
    "\n",
    "# transforming the torch tensors to a dataset\n",
    "trainset = torch.utils.data.TensorDataset(tensor_X_train, tensor_Y_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(trainset, batch_size, shuffle=False)\n",
    "\n",
    "tensor_X_val = torch.Tensor(x_val)\n",
    "tensor_Y_val = torch.Tensor(y_val).type(torch.LongTensor)\n",
    "\n",
    "valset = torch.utils.data.TensorDataset(tensor_X_val, tensor_Y_val)\n",
    "val_dataloader = torch.utils.data.DataLoader(valset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# # *****START CODE\n",
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         #starting image size = 64X64\n",
    "#         self.conv1 = nn.Conv2d(3, 16, 3, padding=(1,1)) # in_channels, out_channels, kernel_size\n",
    "#         self.conv2 = nn.Conv2d(16,16,3, padding=(1,1))\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2)\n",
    "#         self.dropout1 = nn.Dropout2d(0.1)\n",
    "#         self.conv3 = nn.Conv2d(16, 32, 3, padding=(1,1))\n",
    "#         self.conv4 = nn.Conv2d(32, 32, 3, padding=(1,1))\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2)\n",
    "#         self.dropout2 = nn.Dropout2d(0.1)\n",
    "\n",
    "# #FLATTENING\n",
    "#         self.fc1 = nn.Linear(32 * 64 * 64, 512)\n",
    "#         self.dropout3 = nn.Dropout(p=0.3)\n",
    "#         self.fc2 = nn.Linear(512, 256)\n",
    "#         self.fc3 = nn.Linear(256, 27)\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.dropout1(self.pool1(F.relu(self.conv2(x))))\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.dropout2(self.pool2(F.relu(self.conv4(x))))\n",
    "#         x = x.view(-1, 32 * 64 * 64)\n",
    "#         x = F.relu(self.dropout3(self.fc1(x)))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "# class ConvNet(nn.Module): # Best so far\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 96, kernel_size=3,stride=1,padding=1)   # 3X64X64 to 96X64X64       \n",
    "#         self.conv2 = nn.Conv2d(96, 96, kernel_size=3,stride=1,padding=1)  # 96X64X64 to 96X64X64                 \n",
    "#         self.conv3 = nn.Conv2d(96, 96, kernel_size=3,stride=2,padding=1)  # 96X64X64 to 96X32X32                 \n",
    "#         self.conv4 = nn.Conv2d(96, 192, kernel_size=3,stride=1,padding=1)  # 96X32X32 to 192X32X32                 \n",
    "#         self.conv5 = nn.Conv2d(192, 192, kernel_size=3,stride=1,padding=1)   \n",
    "#         self.conv6 = nn.Conv2d(192, 192, kernel_size=3,stride=2,padding=1)  # 192X32X32 to 192X16X16                 \n",
    "#         self.conv7 = nn.Conv2d(192, 192, kernel_size=3,stride=1,padding=0)  # 192X16X16 to 192X14X14\n",
    "#         self.conv8 = nn.Conv2d(192, 10, kernel_size=1,stride=1,padding=0)   # 192X14X14 to 10X14X14    \n",
    "#         self.conv9 = nn.Conv2d(10, 10, kernel_size=1,stride=1,padding=0)    # 10X14X14 to 10X14X14 \n",
    "#         self.dropout = nn.Dropout(p=.5)\n",
    "#         # self.pool = nn.AvgPool2d(kernel_size=6, stride=1, padding=0) \n",
    "#         self.fc1 = nn.Linear(10*14*14,27)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = self.dropout(F.relu(self.conv3(x)))\n",
    "#         x = F.relu(self.conv4(x))\n",
    "#         x = F.relu(self.conv5(x))\n",
    "#         x = self.dropout(F.relu(self.conv6(x)))\n",
    "#         x = F.relu(self.conv7(x))\n",
    "#         x = F.relu(self.conv8(x))\n",
    "#         x = F.relu(self.conv9(x))\n",
    "#         x = x.view(-1, 10*14*14)\n",
    "#         x = self.fc1(x)\n",
    "#         return x\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        #starting image size = 128x128\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=(1,1)) # in_channels, out_channels, kernel_size\n",
    "        self.conv2 = nn.Conv2d(16,40,3, padding=(1,1))\n",
    "        self.conv3 = nn.Conv2d(40, 70, 3, padding=(1,1))\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout2d(0.2)\n",
    "        self.dropout2 = nn.Dropout2d(0.4)\n",
    "\n",
    "        self.fc1 = nn.Linear(70 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 27)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.dropout1(self.pool(F.relu(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 70 * 16 * 16)\n",
    "        x = F.relu(self.dropout2(self.fc1(x)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = ConvNet()\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 32\n",
      "epochs= 10\n",
      "learning_rate= 0.001\n",
      "==============================\n",
      "Epoch 1, 6% \t train_loss: 8.28 took: 10.30s\n",
      "Epoch 1, 12% \t train_loss: 3.31 took: 9.89s\n",
      "Epoch 1, 18% \t train_loss: 3.35 took: 9.91s\n",
      "Epoch 1, 24% \t train_loss: 3.30 took: 10.06s\n",
      "Epoch 1, 31% \t train_loss: 3.31 took: 10.29s\n",
      "Epoch 1, 37% \t train_loss: 3.28 took: 10.05s\n",
      "Epoch 1, 43% \t train_loss: 3.32 took: 10.03s\n",
      "Epoch 1, 49% \t train_loss: 3.29 took: 10.14s\n",
      "Epoch 1, 55% \t train_loss: 3.27 took: 10.70s\n",
      "Epoch 1, 62% \t train_loss: 3.20 took: 10.03s\n",
      "Epoch 1, 68% \t train_loss: 3.18 took: 10.05s\n",
      "Epoch 1, 74% \t train_loss: 3.24 took: 10.17s\n",
      "Epoch 1, 80% \t train_loss: 3.13 took: 9.91s\n",
      "Epoch 1, 86% \t train_loss: 3.06 took: 9.94s\n",
      "Epoch 1, 93% \t train_loss: 2.86 took: 10.04s\n",
      "Epoch 1, 99% \t train_loss: 2.72 took: 10.32s\n",
      "Validation loss = 2.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ConvNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, 6% \t train_loss: 3.60 took: 11.14s\n",
      "Epoch 2, 12% \t train_loss: 3.36 took: 10.53s\n",
      "Epoch 2, 18% \t train_loss: 3.33 took: 10.56s\n",
      "Epoch 2, 24% \t train_loss: 3.32 took: 10.82s\n",
      "Epoch 2, 31% \t train_loss: 3.31 took: 10.36s\n",
      "Epoch 2, 37% \t train_loss: 3.28 took: 9.68s\n",
      "Epoch 2, 43% \t train_loss: 3.35 took: 9.63s\n",
      "Epoch 2, 49% \t train_loss: 3.31 took: 9.53s\n",
      "Epoch 2, 55% \t train_loss: 3.31 took: 9.57s\n",
      "Epoch 2, 62% \t train_loss: 3.22 took: 9.56s\n",
      "Epoch 2, 68% \t train_loss: 3.23 took: 9.65s\n",
      "Epoch 2, 74% \t train_loss: 3.21 took: 9.62s\n",
      "Epoch 2, 80% \t train_loss: 3.16 took: 9.61s\n",
      "Epoch 2, 86% \t train_loss: 2.92 took: 9.68s\n",
      "Epoch 2, 93% \t train_loss: 2.76 took: 9.61s\n",
      "Epoch 2, 99% \t train_loss: 2.56 took: 9.88s\n",
      "Validation loss = 2.86\n",
      "Epoch 3, 6% \t train_loss: 3.68 took: 10.34s\n",
      "Epoch 3, 12% \t train_loss: 3.31 took: 11.03s\n",
      "Epoch 3, 18% \t train_loss: 3.28 took: 11.37s\n",
      "Epoch 3, 24% \t train_loss: 3.22 took: 11.45s\n",
      "Epoch 3, 31% \t train_loss: 3.13 took: 11.75s\n",
      "Epoch 3, 37% \t train_loss: 3.21 took: 11.74s\n",
      "Epoch 3, 43% \t train_loss: 3.16 took: 11.59s\n",
      "Epoch 3, 49% \t train_loss: 3.11 took: 11.62s\n",
      "Epoch 3, 55% \t train_loss: 3.04 took: 11.28s\n",
      "Epoch 3, 62% \t train_loss: 2.92 took: 10.87s\n",
      "Epoch 3, 68% \t train_loss: 2.99 took: 10.74s\n",
      "Epoch 3, 74% \t train_loss: 2.96 took: 10.68s\n",
      "Epoch 3, 80% \t train_loss: 2.82 took: 10.55s\n",
      "Epoch 3, 86% \t train_loss: 2.68 took: 10.60s\n",
      "Epoch 3, 93% \t train_loss: 2.59 took: 10.63s\n",
      "Epoch 3, 99% \t train_loss: 2.45 took: 10.86s\n",
      "Validation loss = 2.80\n",
      "Epoch 4, 6% \t train_loss: 3.62 took: 11.04s\n",
      "Epoch 4, 12% \t train_loss: 3.32 took: 11.04s\n",
      "Epoch 4, 18% \t train_loss: 3.31 took: 10.83s\n",
      "Epoch 4, 24% \t train_loss: 3.29 took: 10.82s\n",
      "Epoch 4, 31% \t train_loss: 3.24 took: 10.79s\n",
      "Epoch 4, 37% \t train_loss: 3.26 took: 10.68s\n",
      "Epoch 4, 43% \t train_loss: 3.24 took: 10.88s\n",
      "Epoch 4, 49% \t train_loss: 3.19 took: 10.88s\n",
      "Epoch 4, 55% \t train_loss: 3.16 took: 10.61s\n",
      "Epoch 4, 62% \t train_loss: 3.04 took: 10.67s\n",
      "Epoch 4, 68% \t train_loss: 2.97 took: 10.59s\n",
      "Epoch 4, 74% \t train_loss: 2.95 took: 10.66s\n",
      "Epoch 4, 80% \t train_loss: 2.73 took: 10.56s\n",
      "Epoch 4, 86% \t train_loss: 2.62 took: 10.75s\n",
      "Epoch 4, 93% \t train_loss: 2.48 took: 10.75s\n",
      "Epoch 4, 99% \t train_loss: 2.49 took: 10.55s\n",
      "Validation loss = 2.83\n",
      "Epoch 5, 6% \t train_loss: 3.54 took: 11.46s\n",
      "Epoch 5, 12% \t train_loss: 3.23 took: 11.90s\n",
      "Epoch 5, 18% \t train_loss: 3.10 took: 12.86s\n",
      "Epoch 5, 24% \t train_loss: 3.00 took: 11.96s\n",
      "Epoch 5, 31% \t train_loss: 2.94 took: 11.03s\n",
      "Epoch 5, 37% \t train_loss: 2.91 took: 10.93s\n",
      "Epoch 5, 43% \t train_loss: 2.85 took: 13.45s\n",
      "Epoch 5, 49% \t train_loss: 2.81 took: 13.88s\n",
      "Epoch 5, 55% \t train_loss: 2.79 took: 13.56s\n",
      "Epoch 5, 62% \t train_loss: 2.66 took: 11.05s\n",
      "Epoch 5, 68% \t train_loss: 2.67 took: 11.37s\n",
      "Epoch 5, 74% \t train_loss: 2.68 took: 10.72s\n",
      "Epoch 5, 80% \t train_loss: 2.53 took: 10.81s\n",
      "Epoch 5, 86% \t train_loss: 2.41 took: 11.26s\n",
      "Epoch 5, 93% \t train_loss: 2.34 took: 11.15s\n",
      "Epoch 5, 99% \t train_loss: 2.23 took: 12.11s\n",
      "Validation loss = 2.68\n",
      "Epoch 6, 6% \t train_loss: 3.46 took: 11.69s\n",
      "Epoch 6, 12% \t train_loss: 2.99 took: 10.87s\n",
      "Epoch 6, 18% \t train_loss: 2.82 took: 11.08s\n",
      "Epoch 6, 24% \t train_loss: 2.77 took: 10.89s\n",
      "Epoch 6, 31% \t train_loss: 2.61 took: 10.66s\n",
      "Epoch 6, 37% \t train_loss: 2.63 took: 10.98s\n",
      "Epoch 6, 43% \t train_loss: 2.64 took: 10.94s\n",
      "Epoch 6, 49% \t train_loss: 2.56 took: 10.53s\n",
      "Epoch 6, 55% \t train_loss: 2.49 took: 10.56s\n",
      "Epoch 6, 62% \t train_loss: 2.43 took: 10.68s\n",
      "Epoch 6, 68% \t train_loss: 2.47 took: 10.76s\n",
      "Epoch 6, 74% \t train_loss: 2.45 took: 10.80s\n",
      "Epoch 6, 80% \t train_loss: 2.25 took: 10.54s\n",
      "Epoch 6, 86% \t train_loss: 2.24 took: 10.67s\n",
      "Epoch 6, 93% \t train_loss: 2.23 took: 10.70s\n",
      "Epoch 6, 99% \t train_loss: 1.98 took: 10.84s\n",
      "Validation loss = 2.81\n",
      "Epoch 7, 6% \t train_loss: 3.22 took: 10.46s\n",
      "Epoch 7, 12% \t train_loss: 2.66 took: 11.20s\n",
      "Epoch 7, 18% \t train_loss: 2.47 took: 10.55s\n",
      "Epoch 7, 24% \t train_loss: 2.47 took: 10.43s\n",
      "Epoch 7, 31% \t train_loss: 2.32 took: 10.37s\n",
      "Epoch 7, 37% \t train_loss: 2.37 took: 10.26s\n",
      "Epoch 7, 43% \t train_loss: 2.37 took: 10.47s\n",
      "Epoch 7, 49% \t train_loss: 2.24 took: 10.54s\n",
      "Epoch 7, 55% \t train_loss: 2.20 took: 10.26s\n",
      "Epoch 7, 62% \t train_loss: 2.13 took: 10.24s\n",
      "Epoch 7, 68% \t train_loss: 2.18 took: 10.22s\n",
      "Epoch 7, 74% \t train_loss: 2.17 took: 10.32s\n",
      "Epoch 7, 80% \t train_loss: 2.01 took: 10.22s\n",
      "Epoch 7, 86% \t train_loss: 1.96 took: 10.25s\n",
      "Epoch 7, 93% \t train_loss: 1.84 took: 10.37s\n",
      "Epoch 7, 99% \t train_loss: 1.82 took: 10.44s\n",
      "Validation loss = 2.86\n",
      "Epoch 8, 6% \t train_loss: 2.98 took: 10.46s\n",
      "Epoch 8, 12% \t train_loss: 2.33 took: 10.61s\n",
      "Epoch 8, 18% \t train_loss: 2.18 took: 10.61s\n",
      "Epoch 8, 24% \t train_loss: 2.06 took: 10.61s\n",
      "Epoch 8, 31% \t train_loss: 2.01 took: 10.70s\n",
      "Epoch 8, 37% \t train_loss: 1.97 took: 10.23s\n",
      "Epoch 8, 43% \t train_loss: 1.97 took: 10.40s\n",
      "Epoch 8, 49% \t train_loss: 1.95 took: 10.45s\n",
      "Epoch 8, 55% \t train_loss: 1.91 took: 10.27s\n",
      "Epoch 8, 62% \t train_loss: 1.92 took: 10.29s\n",
      "Epoch 8, 68% \t train_loss: 1.88 took: 10.52s\n",
      "Epoch 8, 74% \t train_loss: 1.89 took: 10.46s\n",
      "Epoch 8, 80% \t train_loss: 1.77 took: 10.54s\n",
      "Epoch 8, 86% \t train_loss: 1.49 took: 10.43s\n",
      "Epoch 8, 93% \t train_loss: 1.43 took: 10.49s\n",
      "Epoch 8, 99% \t train_loss: 1.45 took: 10.41s\n",
      "Validation loss = 3.12\n",
      "Epoch 9, 6% \t train_loss: 2.65 took: 11.11s\n",
      "Epoch 9, 12% \t train_loss: 2.05 took: 10.89s\n",
      "Epoch 9, 18% \t train_loss: 1.85 took: 10.88s\n",
      "Epoch 9, 24% \t train_loss: 1.81 took: 10.93s\n",
      "Epoch 9, 31% \t train_loss: 1.66 took: 11.75s\n",
      "Epoch 9, 37% \t train_loss: 1.67 took: 11.03s\n",
      "Epoch 9, 43% \t train_loss: 1.73 took: 11.05s\n",
      "Epoch 9, 49% \t train_loss: 1.64 took: 10.61s\n",
      "Epoch 9, 55% \t train_loss: 1.62 took: 10.74s\n",
      "Epoch 9, 62% \t train_loss: 1.54 took: 10.77s\n",
      "Epoch 9, 68% \t train_loss: 1.56 took: 10.31s\n",
      "Epoch 9, 74% \t train_loss: 1.58 took: 10.33s\n",
      "Epoch 9, 80% \t train_loss: 1.57 took: 10.48s\n",
      "Epoch 9, 86% \t train_loss: 1.23 took: 10.29s\n",
      "Epoch 9, 93% \t train_loss: 1.20 took: 10.22s\n",
      "Epoch 9, 99% \t train_loss: 1.11 took: 10.41s\n",
      "Validation loss = 3.68\n",
      "Epoch 10, 6% \t train_loss: 2.30 took: 10.57s\n",
      "Epoch 10, 12% \t train_loss: 1.76 took: 10.25s\n",
      "Epoch 10, 18% \t train_loss: 1.57 took: 10.19s\n",
      "Epoch 10, 24% \t train_loss: 1.46 took: 10.38s\n",
      "Epoch 10, 31% \t train_loss: 1.40 took: 10.35s\n",
      "Epoch 10, 37% \t train_loss: 1.44 took: 10.22s\n",
      "Epoch 10, 43% \t train_loss: 1.41 took: 10.48s\n",
      "Epoch 10, 49% \t train_loss: 1.50 took: 10.47s\n",
      "Epoch 10, 55% \t train_loss: 1.45 took: 10.25s\n",
      "Epoch 10, 62% \t train_loss: 1.34 took: 10.16s\n",
      "Epoch 10, 68% \t train_loss: 1.37 took: 10.54s\n",
      "Epoch 10, 74% \t train_loss: 1.33 took: 10.30s\n",
      "Epoch 10, 80% \t train_loss: 1.25 took: 10.39s\n",
      "Epoch 10, 86% \t train_loss: 1.11 took: 10.81s\n",
      "Epoch 10, 93% \t train_loss: 0.97 took: 10.75s\n",
      "Epoch 10, 99% \t train_loss: 1.08 took: 11.15s\n",
      "Validation loss = 3.65\n",
      "Training finished, took 1867.45s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "#Print all of the hyperparameters of the training iteration:\n",
    "print(\"===== HYPERPARAMETERS =====\")\n",
    "print(\"batch_size=\", batch_size)\n",
    "print(\"epochs=\", epochs)\n",
    "print(\"learning_rate=\", learning_rate)\n",
    "print(\"=\" * 30)\n",
    "\n",
    "n_batches = len(train_dataloader)\n",
    "training_start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "\n",
    "    running_loss = 0.0\n",
    "    print_every = 20 # 10\n",
    "    start_time = time.time()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "\n",
    "        #Get inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        #Wrap them in a Variable object\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        #Set the parameter gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Forward pass, backward pass, optimize\n",
    "        outputs = model(inputs)\n",
    "        loss_size = criterion(outputs, labels)\n",
    "        loss_size.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss_size.data\n",
    "        total_train_loss += loss_size.data\n",
    "\n",
    "        #Print every 10th batch of an epoch\n",
    "        if (i + 1) % (print_every + 1) == 0:\n",
    "            print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                    epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
    "            #Reset running loss and time\n",
    "            running_loss = 0.0\n",
    "            start_time = time.time()\n",
    "\n",
    "    #At the end of the epoch, do a pass on the validation set\n",
    "    total_val_loss = 0\n",
    "    for inputs, labels in val_dataloader:\n",
    "\n",
    "        #Wrap tensors in Variables\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        #Forward pass\n",
    "        val_outputs = model(inputs)\n",
    "        val_loss_size = criterion(val_outputs, labels)\n",
    "        total_val_loss += val_loss_size.data\n",
    "\n",
    "    train_loss_list.append(total_train_loss)\n",
    "    val_loss_list.append(total_val_loss)\n",
    "\n",
    "    print(\"Validation loss = {:.2f}\".format(total_val_loss / len(val_dataloader)))\n",
    "\n",
    "\n",
    "    torch.save(model, 'model-elta-epoch-3.pkl')\n",
    "\n",
    "print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_list = [x/len(train_dataloader) for x in train_loss_list]\n",
    "val_loss_list = [x/len(val_dataloader) for x in val_loss_list]\n",
    "\n",
    "plt.plot(train_loss_list,label='train_loss')\n",
    "plt.plot(val_loss_list,label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "net = torch.load('model-elta-epoch-3.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance on Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the validation set images: 23 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in val_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the validation set images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance on Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the train set images: 33 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the train set images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMAGES_PATH = './images/image_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>imagefilename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>84916</td>\n",
       "      <td>Folkmanis Puppets - 2732 - Marionnette Et Théâ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>516376098</td>\n",
       "      <td>1019294171</td>\n",
       "      <td>image_1019294171_product_516376098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>84917</td>\n",
       "      <td>Porte Flamme Gaxix - Flamebringer Gaxix - 136/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133389013</td>\n",
       "      <td>1274228667</td>\n",
       "      <td>image_1274228667_product_133389013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>84918</td>\n",
       "      <td>Pompe de filtration Speck Badu 95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4128438366</td>\n",
       "      <td>1295960357</td>\n",
       "      <td>image_1295960357_product_4128438366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>84919</td>\n",
       "      <td>Robot de piscine électrique</td>\n",
       "      <td>&lt;p&gt;Ce robot de piscine d&amp;#39;un design innovan...</td>\n",
       "      <td>3929899732</td>\n",
       "      <td>1265224052</td>\n",
       "      <td>image_1265224052_product_3929899732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>84920</td>\n",
       "      <td>Hsm Destructeur Securio C16 Coupe Crois¿E: 4 X...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152993898</td>\n",
       "      <td>940543690</td>\n",
       "      <td>image_940543690_product_152993898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                        designation  \\\n",
       "0  84916  Folkmanis Puppets - 2732 - Marionnette Et Théâ...   \n",
       "1  84917  Porte Flamme Gaxix - Flamebringer Gaxix - 136/...   \n",
       "2  84918                  Pompe de filtration Speck Badu 95   \n",
       "3  84919                        Robot de piscine électrique   \n",
       "4  84920  Hsm Destructeur Securio C16 Coupe Crois¿E: 4 X...   \n",
       "\n",
       "                                         description   productid     imageid  \\\n",
       "0                                                NaN   516376098  1019294171   \n",
       "1                                                NaN   133389013  1274228667   \n",
       "2                                                NaN  4128438366  1295960357   \n",
       "3  <p>Ce robot de piscine d&#39;un design innovan...  3929899732  1265224052   \n",
       "4                                                NaN   152993898   940543690   \n",
       "\n",
       "                         imagefilename  \n",
       "0   image_1019294171_product_516376098  \n",
       "1   image_1274228667_product_133389013  \n",
       "2  image_1295960357_product_4128438366  \n",
       "3  image_1265224052_product_3929899732  \n",
       "4    image_940543690_product_152993898  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_csv('X_test.csv', header=0, names=['id','designation','description','productid','imageid'])\n",
    "X_test['imagefilename'] = X_test.apply(lambda x: 'image_'+str(x['imageid'])+'_product_'+str(x['productid']), axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c490cd51f7124dc9965ca3d6e0bc4dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# X_test_subsampled = X_test[:100]\n",
    "X_test_subsampled = X_test\n",
    "\n",
    "image_filenames = [f for f in X_test_subsampled['imagefilename']]\n",
    "sample_length = len(X_test_subsampled)\n",
    "\n",
    "test_images=np.zeros((sample_length,IMG_HEIGHT,IMG_WIDTH,3))\n",
    "\n",
    "for i,fi in tqdm(enumerate(image_filenames)):\n",
    "    pic = cv2.imread(TEST_IMAGES_PATH + fi + '.jpg')\n",
    "    pic = cv2.resize(pic,(IMG_HEIGHT,IMG_WIDTH))\n",
    "    test_images[i] = pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13812, 3, 128, 128)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images = np.moveaxis(test_images, -1, 1)\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# transforming the numpy array to a torch tensor\n",
    "tensor_X_test = torch.Tensor(test_images)\n",
    "\n",
    "# transforming the torch tensors to a dataset\n",
    "testset = torch.utils.data.TensorDataset(tensor_X_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(testset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([])\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images = data[0]\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions = np.concatenate((predictions,predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(predictions).to_csv('images23%.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ELTA_CNN_V2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ebbb94d50044287a334ced4152c9bb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "181faed42823418dab4816897baa05de": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1bf83475115e4f1c9bcf356ca59858c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_181faed42823418dab4816897baa05de",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0ebbb94d50044287a334ced4152c9bb7",
      "value": 1
     }
    },
    "54c4065184f64c54b969a8a4ad41d9b4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6987cd88720246f386ea3736f2d8de77": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "948306d203864f68b066ebccd2fca7c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d54efb313523409ca04e0403c4cf3eeb",
      "placeholder": "​",
      "style": "IPY_MODEL_6987cd88720246f386ea3736f2d8de77",
      "value": "170500096it [00:04, 42131493.94it/s]"
     }
    },
    "a5bd25f3db074a5e9b036d1b9ae2bb5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1bf83475115e4f1c9bcf356ca59858c5",
       "IPY_MODEL_948306d203864f68b066ebccd2fca7c1"
      ],
      "layout": "IPY_MODEL_54c4065184f64c54b969a8a4ad41d9b4"
     }
    },
    "d54efb313523409ca04e0403c4cf3eeb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
